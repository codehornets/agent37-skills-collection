# How To Train An LLM with Anthropic's Head of Pretraining

**Author:** Y Combinator
**Type:** Video
**URL:** https://www.ycombinator.com/library/Mw-how-to-train-an-llm-with-anthropic-s-head-of-pretraining

---

How To Train An LLM with Anthropic's Head of Pretraining

Main Function

# How To Train An LLM with Anthropic's Head of Pretraining

by Y Combinator

Anthropic Head of Pretraining on Scaling Laws, Compute, and the Future of AI - YouTube

[Photo image of Y Combinator](https://www.youtube.com/channel/UCcefcZRL2oaA_uBNeo5UOWg?embeds_referring_euri=https%3A%2F%2Fwww.ycombinator.com%2F&embeds_referring_origin=https%3A%2F%2Fwww.ycombinator.com)

Y Combinator

2.1M subscribers

[Anthropic Head of Pretraining on Scaling Laws, Compute, and the Future of AI](https://www.youtube.com/watch?v=YFeb3yAxtjE)

Y Combinator

Search

Info

Shopping

Tap to unmute

If playback doesn't begin shortly, try restarting your device.

You're signed out

Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.

CancelConfirm

Up NextCancelAutoplay is paused

Share

Include playlist

An error occurred while retrieving sharing information. Please try again later.

Watch later

Share

Copy link

Watch on

0:00

0:00 / 1:04:05

•Watch full videoLive

•
Introduction

27K views

3 months ago

## Chapters

0:00

– Introduction

1:05

– From Vicarious to OpenAI to Anthropic

6:40

– What pretraining is

11:20

– Why next-word prediction won out

16:05

– Scaling laws and the feedback loop of compute → models → revenue

21:50

– Building Anthropic’s early infrastructure

27:35

– Efficiency hacks and debugging at scale

33:10

– Generalists vs. specialists on the pretraining team

38:45

– Challenges of training across thousands of GPUs

44:15

– Working with new chips: GPUs vs. TPUs

49:00

– Pretraining vs. post-training (RLHF and reasoning models)

54:25

– The future of data quality and availability

59:10

– Where pretraining goes next

1:03:00

– Closing reflections

Ever wonder what it actually takes to train a frontier AI model?

Ankit Gupta, YC General Partner, sits down with Nick Joseph, Anthropic's Head of Pre-training, to explore the
engineering challenges behind training Claude—from managing thousands of GPUs and debugging cursed bugs to balancing
compute between pre-training and RL. We cover scaling laws, data strategies, team composition, and why the hardest
problems in AI are often infrastructure problems, not ML problems.
